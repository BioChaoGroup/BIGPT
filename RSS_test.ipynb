{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 瘦身代码,先用存入本地的html测试\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#with open(\"nature.html\") as f:\n",
    "with open(\"science.html\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "localhtml = html\n",
    "souphtml = BeautifulSoup(html, \"html.parser\")\n",
    "for i in souphtml.find_all(True):\n",
    "    for name in list(i.attrs):\n",
    "        if i[name] and name not in [\"class\"]:\n",
    "            del i[name]\n",
    "\n",
    "for i in souphtml.find_all([\"svg\", \"img\", \"video\", \"audio\"]):\n",
    "    i.decompose()\n",
    "\n",
    "with open(\"science-slim.html\", \"w\") as f:\n",
    "    f.write(str(souphtml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你是一个精通python的爬虫工程师，需要使用aiohttp爬取网页，然后用BeautifulSoup解析出列表中的几个字段：几个字段：文章名，摘要内容, url，doi, 发表日期, 发表杂志\n",
    "\n",
    "目标html代码有如下结构:\n",
    "```html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def fetch_page(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def scrape_article_data(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch_page(session, url)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        articles_data = []\n",
    "\n",
    "        # Find all article cards\n",
    "        cards = soup.find_all('div', class_='card-content')\n",
    "\n",
    "        for card in cards:\n",
    "            # Extracting required fields\n",
    "            title_tag = card.find('h3', class_='article-title')\n",
    "\n",
    "            if title_tag and title_tag.find('a'):\n",
    "                article_title = title_tag.get_text(strip=True)\n",
    "                article_url = title_tag.find('a')['href']\n",
    "                article_doi = article_url.split('/doi/')[-1] if '/doi/' in article_url else 'DOI not found'\n",
    "\n",
    "                body_tag = card.find('div', class_='card-body')\n",
    "                article_abstract = body_tag.get_text(strip=True) if body_tag else 'Abstract not found'\n",
    "\n",
    "                time_tag = card.find('time')\n",
    "                publication_date = time_tag.get_text(strip=True) if time_tag else 'Date not found'\n",
    "\n",
    "                # Assuming the magazine title comes after \"In\" within the article title\n",
    "                publishing_magazine = article_title.split(' In ')[-1] if ' In ' in article_title else 'Magazine not found'\n",
    "\n",
    "                articles_data.append({\n",
    "                    'Article Name': article_title,\n",
    "                    'URL': article_url,\n",
    "                    'DOI': article_doi,\n",
    "                    'Abstract Content': article_abstract,\n",
    "                    'Publishing Date': publication_date,\n",
    "                    'Publishing Magazine': publishing_magazine\n",
    "                })\n",
    "\n",
    "        return articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Article Name': 'Time to support Indigenous science', 'URL': '/doi/10.1126/science.ado0684', 'DOI': '10.1126/science.ado0684', 'Abstract Content': 'Abstract not found', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'News at a glance', 'URL': '/doi/10.1126/science.ado0966', 'DOI': '10.1126/science.ado0966', 'Abstract Content': 'Abstract not found', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Uprooted Ukrainian academics reboot in exile', 'URL': '/doi/10.1126/science.ado0967', 'DOI': '10.1126/science.ado0967', 'Abstract Content': 'Many institutions and scientists displaced from occupied territories may never return', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Bacteria stitch exotic building blocks into novel proteins', 'URL': '/doi/10.1126/science.ado0968', 'DOI': '10.1126/science.ado0968', 'Abstract Content': 'Efficient method for creating proteins with unusual amino acids opens the door to new medicines and catalysts', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'EPA scraps plan to end all testing in mammals by 2035', 'URL': '/doi/10.1126/science.ado0969', 'DOI': '10.1126/science.ado0969', 'Abstract Content': 'U.S. environmental agency’s hard deadline had split scientific community', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'A mammoth’s life story, written in tusk', 'URL': '/doi/10.1126/science.ado0970', 'DOI': '10.1126/science.ado0970', 'Abstract Content': 'The travels of “Elma” show she faced twin pressures—climate change and human hunting', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Wisconsin bill would restrict pathogen studies', 'URL': '/doi/10.1126/science.ado0972', 'DOI': '10.1126/science.ado0972', 'Abstract Content': 'Efforts to ban “gain-of-function” research on viruses and bacteria worry scientists', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Second image of ‘shadow’ confirms giant black hole is real', 'URL': '/doi/10.1126/science.ado0973', 'DOI': '10.1126/science.ado0973', 'Abstract Content': 'To zoom in farther, Event Horizon Telescope wants to add more radio dishes to its network—and go to space', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Paper trail', 'URL': '/doi/10.1126/science.ado0309', 'DOI': '10.1126/science.ado0309', 'Abstract Content': 'In the latest twist of the publishing arms race, firms churning out fake papers have taken to bribing journal editors', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Prion science and its unsung heroes', 'URL': '/doi/10.1126/science.adn9424', 'DOI': '10.1126/science.adn9424', 'Abstract Content': 'Abstract not found', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Mystery in the “mass gap”', 'URL': '/doi/10.1126/science.adn1869', 'DOI': '10.1126/science.adn1869', 'Abstract Content': 'The identity of a compact astronomical object in the Milky Way is unclear', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Surpassing sensitivity limits in liquid biopsy', 'URL': '/doi/10.1126/science.adn1886', 'DOI': '10.1126/science.adn1886', 'Abstract Content': 'Attenuation of cell-free DNA clearance in vivo is an alternative strategy to maximize recovery', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Immune damage in Long Covid', 'URL': '/doi/10.1126/science.adn1077', 'DOI': '10.1126/science.adn1077', 'Abstract Content': 'Links between the complement and coagulation systems could lead to Long Covid therapies', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Qubits without qubits', 'URL': '/doi/10.1126/science.adm9946', 'DOI': '10.1126/science.adm9946', 'Abstract Content': 'Light-based platforms for quantum computing do not require physical qubits', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Risks from solar-powered groundwater irrigation', 'URL': '/doi/10.1126/science.adi9497', 'DOI': '10.1126/science.adi9497', 'Abstract Content': 'Emissions reductions may not meet expectations, and groundwater use will likely increase', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Searching for meaning in a world aflame', 'URL': '/doi/10.1126/science.adm9750', 'DOI': '10.1126/science.adm9750', 'Abstract Content': 'A metaphor-filled memoir bears witness to our fraught existence in the age of wildfires', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'When the CIA met LSD', 'URL': '/doi/10.1126/science.adm9748', 'DOI': '10.1126/science.adm9748', 'Abstract Content': 'A pair of anthropologists’ personal papers shed light on US psychedelic science in the mid-20th century', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Pig virus imperils food security in Borneo', 'URL': '/doi/10.1126/science.adn3857', 'DOI': '10.1126/science.adn3857', 'Abstract Content': 'Abstract not found', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Environmental risks and infertility in China', 'URL': '/doi/10.1126/science.adn3214', 'DOI': '10.1126/science.adn3214', 'Abstract Content': 'Abstract not found', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'India’s coal mining plan undermines climate goals', 'URL': '/doi/10.1126/science.adn3642', 'DOI': '10.1126/science.adn3642', 'Abstract Content': 'Abstract not found', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'In Science Journals', 'URL': '/doi/10.1126/science.ado0307', 'DOI': '10.1126/science.ado0307', 'Abstract Content': 'Highlights from theSciencefamily of journals', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'In Other Journals', 'URL': '/doi/10.1126/science.ado0308', 'DOI': '10.1126/science.ado0308', 'Abstract Content': 'Editors’ selections from the current scientific literature', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Priming agents transiently reduce the clearance of cell-free DNA to improve liquid biopsies', 'URL': '/doi/10.1126/science.adf2341', 'DOI': '10.1126/science.adf2341', 'Abstract Content': 'Intravenous priming agents given 1 to 2 hours before a blood draw increase circulating tumor DNA recovery by 19- to 60-fold.', 'Publishing Date': '19 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Persistent complement dysregulation with signs of thromboinflammation in active Long Covid', 'URL': '/doi/10.1126/science.adg7942', 'DOI': '10.1126/science.adg7942', 'Abstract Content': 'Analysis of blood revealed that Long COVID is characterized by changes to complement proteins and platelet activation.', 'Publishing Date': '19 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'A pulsar in a binary with a compact object in the mass gap between neutron stars and black holes', 'URL': '/doi/10.1126/science.adg3005', 'DOI': '10.1126/science.adg3005', 'Abstract Content': 'Radio timing of a pulsar indicates that it has a binary companion in the mass gap between neutron stars and black holes.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Electrophotocatalytic perfluoroalkylation by LMCT excitation of Ag(II) perfluoroalkyl carboxylates', 'URL': '/doi/10.1126/science.adk4919', 'DOI': '10.1126/science.adk4919', 'Abstract Content': 'Combining photochemistry and electrochemistry enables the addition of perfluoroalkyl groups from carboxylate precursors to arenes.', 'Publishing Date': '14 Dec 2023', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'The mutual neutralization of hydronium and hydroxide', 'URL': '/doi/10.1126/science.adk1950', 'DOI': '10.1126/science.adk1950', 'Abstract Content': 'Mutual neutralization measurements of isolated H3O+and OH−ions revealed competing electron- and proton-transfer mechanisms.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Logical states for fault-tolerant quantum computation with propagating light', 'URL': '/doi/10.1126/science.adk7560', 'DOI': '10.1126/science.adk7560', 'Abstract Content': 'Engineered photonic states required for fault-tolerant optical quantum computing were generated.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'The global distribution of plants used by humans', 'URL': '/doi/10.1126/science.adg8028', 'DOI': '10.1126/science.adg8028', 'Abstract Content': 'Plants used by people are insufficiently protected globally, suggesting a need to preserve biocultural diversity hotspots.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Surface deformations of the 6 February 2023 earthquake sequence, eastern Türkiye', 'URL': '/doi/10.1126/science.adj3770', 'DOI': '10.1126/science.adj3770', 'Abstract Content': 'Surface observations from the 2023 eastern Türkiye earthquake sequence provide insight into complex fault systems.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Supershear triggering and cascading fault ruptures of the 2023 Kahramanmaraş, Türkiye, earthquake doublet', 'URL': '/doi/10.1126/science.adi1519', 'DOI': '10.1126/science.adi1519', 'Abstract Content': 'The Kahramanmaraş earthquake sequence was complex, with geometry and pre-stress contributing to the rupture behavior.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Space-tiled colloidal crystals from DNA-forced shape-complementary polyhedra pairing', 'URL': '/doi/10.1126/science.adj1021', 'DOI': '10.1126/science.adj1021', 'Abstract Content': 'Ordered colloidal superlattices are assembled from polyhedral nanocrystals using flexible DNA and shape complementarity.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Impact of HLA class I functional divergence on HIV control', 'URL': '/doi/10.1126/science.adk0777', 'DOI': '10.1126/science.adk0777', 'Abstract Content': 'A metric of peptide-binding repertoires indicates that greater divergence between HLA allotype pairs enhances HIV control.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'Near-unity NIR phosphorescent quantum yield from a room-temperature solvated metal nanocluster', 'URL': '/doi/10.1126/science.adk6628', 'DOI': '10.1126/science.adk6628', 'Abstract Content': 'A copper-doped gold cluster exhibited ultrafast intersystem crossing and suppressed nonradiative decay after photoexcitation.', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n",
      "{'Article Name': 'My lab’s lighter footprint', 'URL': '/doi/10.1126/science.ado0093', 'DOI': '10.1126/science.ado0093', 'Abstract Content': 'Abstract not found', 'Publishing Date': '18 Jan 2024', 'Publishing Magazine': 'Magazine not found'}\n"
     ]
    }
   ],
   "source": [
    "# The URL of the target webpage (adjust to the actual URL you want to scrape)\n",
    "target_url = \"https://www.science.org/toc/science/current\"\n",
    "\n",
    "# Set up and run the event loop\n",
    "#loop = asyncio.get_event_loop()\n",
    "articles = await scrape_article_data(target_url)\n",
    "for article in articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "async def fetch(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "                assert response.status == 200\n",
    "                return await response.text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_nt = await fetch(\"https://www.nature.com/nature.rss\")\n",
    "xml_ntb = await fetch(\"https://www.nature.com/nbt.rss\")\n",
    "xml_nm = await fetch(\"https://www.nature.com/nmeth.rss\")\n",
    "xml_nc = await fetch(\"https://www.nature.com/ncomms.rss\")\n",
    "#xml_cell = await fetch(\"https://www.cell.com/cell/current.rss\")\n",
    "#xml_cr = await fetch(\"http://www.cell.com/cell-reports/current.rss\")\n",
    "xml_sci = await fetch(\"https://www.science.org/action/showFeed?type=etoc&feed=rss&jc=science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save one to local\n",
    "with open(\"nbt.rss.xml\", \"w\") as f:\n",
    "    f.write(str(xml_ntb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "询问LLM写一个爬虫代码. Prompt:\n",
    "\n",
    "你是一个精通python的爬虫工程师，需要使用aiohttp获取rss订阅源，然后解析出榜单中的几个字段：文章名，摘要内容,对应的url，doi, 发表日期,来源(杂志名)等.\n",
    "该订阅源的xml代码如下:\n",
    "```xml\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from lxml import etree\n",
    "from lxml.html import fromstring\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Parse the HTML content and get the text without tags\n",
    "    tree = fromstring(html_content)\n",
    "    text = tree.text_content().strip()  # Remove leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def parse_rss(feed_url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        xml_data = await fetch(session, feed_url)\n",
    "        root = etree.fromstring(xml_data.encode('utf-8'))\n",
    "\n",
    "        # XML中使用的命名空间\n",
    "        ns_map = {\n",
    "            'rdf': \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "            'prism': \"http://prismstandard.org/namespaces/basic/2.0/\",\n",
    "            'dc': \"http://purl.org/dc/elements/1.1/\",\n",
    "            'content': \"http://purl.org/rss/1.0/modules/content/\",\n",
    "            # 默认命名空间没有前缀，在XPath中必须给它一个前缀\n",
    "            'default': \"http://purl.org/rss/1.0/\",\n",
    "        }\n",
    "\n",
    "        articles = []\n",
    "        for item in root.xpath('//default:item', namespaces=ns_map):\n",
    "            \n",
    "            summary_html = item.findtext('content:encoded', namespaces=ns_map)\n",
    "            # Clean HTML tags from the summary\n",
    "            summary_clean = clean_html(summary_html)\n",
    "\n",
    "            # Splitting into published journal info and description parts\n",
    "            split_summary = summary_clean.split('; ', 1)\n",
    "            journal_info = split_summary[0].strip() if len(split_summary) > 0 else ''\n",
    "            description = split_summary[1].strip() if len(split_summary) > 1 else ''\n",
    "\n",
    "            article = {\n",
    "                'title': item.findtext('dc:title', namespaces=ns_map),\n",
    "                'journal_info': journal_info,\n",
    "                'abstract': description,\n",
    "                'url': item.findtext('prism:url', namespaces=ns_map),\n",
    "                'doi': item.findtext('dc:identifier', namespaces=ns_map),\n",
    "                'publish_date': item.findtext('dc:date', namespaces=ns_map),\n",
    "                'source': item.findtext('prism:publicationName', namespaces=ns_map),\n",
    "            }\n",
    "            articles.append(article)\n",
    "\n",
    "        return articles\n",
    "\n",
    "async def main():\n",
    "    feed_url = 'http://feeds.nature.com/nbt/rss/current'\n",
    "    articles = await parse_rss(feed_url)\n",
    "    for article in articles:\n",
    "        print(article)\n",
    "\n",
    "\n",
    "#asyncio.run(main())\n",
    "articles = await parse_rss('http://feeds.nature.com/nbt/rss/current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                <p>Nature Biotechnology, Published online: 18 January 2024; <a href=\"https://www.nature.com/articles/s41587-023-02110-1\">doi:10.1038/s41587-023-02110-1</a></p>Mapping higher-order RNA structures and intermolecular RNA–RNA interactions throughout the transcriptome is critical for understanding RNA functions. We developed KARR-seq, a chemical-assisted RNA proximity capture and sequencing technology that enables sensitive and accurate detection of the RNA structurome and functional RNA–RNA interactions.\n",
      "Nature Biotechnology, Published online: 18 January 2024; doi:10.1038/s41587-023-02110-1Mapping higher-order RNA structures and intermolecular RNA–RNA interactions throughout the transcriptome is critical for understanding RNA functions. We developed KARR-seq, a chemical-assisted RNA proximity capture and sequencing technology that enables sensitive and accurate detection of the RNA structurome and functional RNA–RNA interactions.\n",
      "doi:10.1038/s41587-023-02110-1Mapping higher-order RNA structures and intermolecular RNA–RNA interactions throughout the transcriptome is critical for understanding RNA functions. We developed KARR-seq, a chemical-assisted RNA proximity capture and sequencing technology that enables sensitive and accurate detection of the RNA structurome and functional RNA–RNA interactions.\n"
     ]
    }
   ],
   "source": [
    "root = etree.fromstring(xml_ntb.encode('utf-8'))\n",
    "\n",
    "\n",
    "# XML中使用的命名空间\n",
    "ns_map = {\n",
    "    'rdf': \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "    'prism': \"http://prismstandard.org/namespaces/basic/2.0/\",\n",
    "    'dc': \"http://purl.org/dc/elements/1.1/\",\n",
    "    'content': \"http://purl.org/rss/1.0/modules/content/\",\n",
    "    # 默认命名空间没有前缀，在XPath中必须给它一个前缀\n",
    "    'default': \"http://purl.org/rss/1.0/\",\n",
    "}\n",
    "\n",
    "items = root.xpath('//default:item', namespaces=ns_map)\n",
    "\n",
    "item = items[0]\n",
    "\n",
    "summary_html = item.findtext('content:encoded', namespaces=ns_map)\n",
    "print(summary_html)\n",
    "# Clean HTML tags from the summary\n",
    "summary_clean = clean_html(summary_html)\n",
    "\n",
    "print(summary_clean)\n",
    "\n",
    "\n",
    "article = articles[0]\n",
    "\n",
    "print(article[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def fetch_rss(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "def parse_rss(xml_data):\n",
    "    soup = BeautifulSoup(xml_data, 'xml')\n",
    "    items = soup.find_all(\"item\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for item in items:\n",
    "        article = {\n",
    "            \"title\": item.title.text,\n",
    "            \"content\": item.title.text,\n",
    "            \"url\": item.link.text,\n",
    "            \"doi\": item.find(\"dc:identifier\").text if item.find(\"dc:identifier\") else None,\n",
    "            \"date_published\": item.find(\"dc:date\").text if item.find(\"dc:date\") else None\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: KARR-seq maps higher-order RNA structures and RNA–RNA interactions across the transcriptome\n",
      "URL: https://www.nature.com/articles/s41587-023-02110-1\n",
      "DOI: doi:10.1038/s41587-023-02110-1\n",
      "Published Date: 2024-01-18\n",
      "------------------------------\n",
      "Title: KARR-seq reveals cellular higher-order RNA structures and RNA–RNA interactions\n",
      "URL: https://www.nature.com/articles/s41587-023-02109-8\n",
      "DOI: doi:10.1038/s41587-023-02109-8\n",
      "Published Date: 2024-01-18\n",
      "------------------------------\n",
      "Title: First self-amplifying mRNA vaccine approved\n",
      "URL: https://www.nature.com/articles/s41587-023-02101-2\n",
      "DOI: doi:10.1038/s41587-023-02101-2\n",
      "Published Date: 2024-01-17\n",
      "------------------------------\n",
      "Title: <i>N</i><sup>1</sup>-methylpseudouridylation affects the fidelity of mRNA translation\n",
      "URL: https://www.nature.com/articles/s41587-023-02113-y\n",
      "DOI: doi:10.1038/s41587-023-02113-y\n",
      "Published Date: 2024-01-17\n",
      "------------------------------\n",
      "Title: Denmark invests in mucosal vaccines\n",
      "URL: https://www.nature.com/articles/s41587-023-02104-z\n",
      "DOI: doi:10.1038/s41587-023-02104-z\n",
      "Published Date: 2024-01-17\n",
      "------------------------------\n",
      "Title: Top biopharmas’ zero-carbon drive\n",
      "URL: https://www.nature.com/articles/s41587-023-02102-1\n",
      "DOI: doi:10.1038/s41587-023-02102-1\n",
      "Published Date: 2024-01-17\n",
      "------------------------------\n",
      "Title: Live cell imaging of signaling networks using a conventional microscope\n",
      "URL: https://www.nature.com/articles/s41587-023-02116-9\n",
      "DOI: doi:10.1038/s41587-023-02116-9\n",
      "Published Date: 2024-01-17\n",
      "------------------------------\n",
      "Title: Biotech news from around the world\n",
      "URL: https://www.nature.com/articles/s41587-023-02093-z\n",
      "DOI: doi:10.1038/s41587-023-02093-z\n",
      "Published Date: 2024-01-17\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def main():\n",
    "    rss_url = \"https://www.nature.com/nbt.rss\"  # Replace with your actual RSS feed URL\n",
    "    xml_data = await fetch_rss(rss_url)\n",
    "    articles = parse_rss(xml_data)\n",
    "\n",
    "    for article in articles:\n",
    "        print(\"Title:\", article[\"title\"])\n",
    "        print(\"URL:\", article[\"url\"])\n",
    "        print(\"DOI:\", article[\"doi\"])\n",
    "        print(\"Published Date:\", article[\"date_published\"])\n",
    "        print(\"---\" * 10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 17:47:30.384 | INFO     | metagpt.const:get_metagpt_package_root:32 - Package root set to /Users/ciao/repo/MetaGPT\n",
      "/Users/ciao/miniforge3/envs/metagpt/lib/python3.9/site-packages/aiofiles/tempfile/temptypes.py:25: RuntimeWarning: coroutine 'ClientResponse.text' was never awaited\n",
      "  class AsyncSpooledTemporaryFile(AsyncBase):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "2024-01-20 17:47:30.576 | INFO     | metagpt.config:get_default_llm_provider_enum:124 - LLMProviderEnum.OPENAI Model: gpt-4-1106-preview\n",
      "2024-01-20 17:47:30.576 | INFO     | metagpt.config:get_default_llm_provider_enum:126 - API: LLMProviderEnum.OPENAI\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from metagpt.actions.action import Action\n",
    "\n",
    "TRENDING_ANALYSIS_PROMPT = \"\"\"# 任务要求\n",
    "你是一位文献调研员. 你被要求基于各大杂志发布的擅长领域和文献标题,汇总生成一篇报告,向用户提供其中的亮点和个性化推荐. \n",
    "\n",
    "内容风格请参考以下大纲:\n",
    "# 今日主刊趋势的标题 (标题应生动并体现今日内容的亮点)\n",
    "## 今日趋势：揭秘今日主刊热研究！探索研究热点领域，并发现吸引开发者注意的关键领域。从**到**，以前所未有的方式见证顶级项目。\n",
    "## 列表亮点：聚焦今日文献标题, 为用户提供独特且引人注目的内容。\n",
    "\n",
    "\n",
    "报告内容请严格按照以下格式生成:\n",
    "```\n",
    "# 今日主刊趋势\n",
    "## 今日趋势\n",
    "今日，**和**继续作为最受欢迎的编程语言占据主导地位。关键兴趣领域包括**、**和**。\n",
    "最受欢迎的项目有Project1和Project2。\n",
    "## 趋势类别\n",
    "1. 生成式AI\n",
    "    - [title1](url)：[项目详情，例如星标总数和今日新增，编程语言，...]\n",
    "    - [title2](url)：...\n",
    "...\n",
    "## 列表亮点\n",
    "1. [title1](url)：[提供推荐此项目的具体原因]。\n",
    "2. [title1](url)：[提供推荐此项目的具体原因]。\n",
    "3. ...\n",
    "```\n",
    "\n",
    "---\n",
    "[主刊最新趋势文本]:\n",
    "{trending}\n",
    "\"\"\"\n",
    "\n",
    "class AnalysisOSSTrending(Action):\n",
    "\n",
    "    async def run(\n",
    "        self,\n",
    "        trending: Any\n",
    "    ):\n",
    "        return await self._aask(TRENDING_ANALYSIS_PROMPT.format(trending=trending))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 17:48:54.651 | INFO     | metagpt.config:get_default_llm_provider_enum:124 - LLMProviderEnum.OPENAI Model: gpt-4-1106-preview\n",
      "2024-01-20 17:48:54.652 | INFO     | metagpt.config:get_default_llm_provider_enum:126 - API: LLMProviderEnum.OPENAI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 今日主刊趋势的标题\n",
      "## 今日趋势：揭秘今日主刊热研究！探索研究热点领域，并发现吸引开发者注意的关键领域。从RNA结构映射到疫苗创新，以前所未有的方式见证顶级项目。\n",
      "\n",
      "## 今日趋势\n",
      "今日，生物科技和医药研究继续作为最受关注的领域占据主导地位。关键兴趣领域包括RNA结构分析、mRNA疫苗技术和可持续生物制药。\n",
      "最受欢迎的项目有KARR-seq技术和自我扩增mRNA疫苗。\n",
      "\n",
      "## 趋势类别\n",
      "1. RNA技术与互作研究\n",
      "    - [KARR-seq maps higher-order RNA structures and RNA–RNA interactions across the transcriptome](https://www.nature.com/articles/s41587-023-02110-1)：[揭示细胞内复杂RNA结构和RNA之间的相互作用，为基因表达调控提供新视角。]\n",
      "    - [KARR-seq reveals cellular higher-order RNA structures and RNA–RNA interactions](https://www.nature.com/articles/s41587-023-02109-8)：[同上，可能是系列研究的一部分，进一步深化了对RNA结构的理解。]\n",
      "2. mRNA疫苗技术\n",
      "    - [First self-amplifying mRNA vaccine approved](https://www.nature.com/articles/s41587-023-02101-2)：[标志着mRNA疫苗技术的一个重大突破，有望改善疫苗的有效性和生产效率。]\n",
      "    - [N1-methylpseudouridylation affects the fidelity of mRNA translation](https://www.nature.com/articles/s41587-023-02113-y)：[探讨了mRNA翻译过程中的一种修饰对翻译准确性的影响，对mRNA疫苗设计有重要意义。]\n",
      "3. 生物制药与可持续性\n",
      "    - [Denmark invests in mucosal vaccines](https://www.nature.com/articles/s41587-023-02104-z)：[丹麦对粘膜疫苗的投资可能推动新一代疫苗的研发，这类疫苗更易于储存和分发。]\n",
      "    - [Top biopharmas’ zero-carbon drive](https://www.nature.com/articles/s41587-023-02102-1)：[生物制药行业的零碳倡议，展示了行业对环境可持续性的承诺。]\n",
      "4. 生物技术工具与方法\n",
      "    - [Live cell imaging of signaling networks using a conventional microscope](https://www.nature.com/articles/s41587-023-02116-9)：[介绍了一种使用常规显微镜实时成像细胞信号网络的新技术，为生物医学研究提供了新工具。]\n",
      "\n",
      "## 列表亮点\n",
      "1. [First self-amplifying mRNA vaccine approved](https://www.nature.com/articles/s41587-023-02101-2)：推荐此项目的原因是它代表了mRNA疫苗技术的一个里程碑，可能会对全球疫苗开发和预防性医疗产生深远影响。\n",
      "2. [KARR-seq maps higher-order RNA structures and RNA–RNA interactions across the transcriptome](https://www.nature.com/articles/s41587-023-02110-1)：推荐此项目的原因是它提供了一种新的技术手段，能够在转录组范围内绘制更高阶的RNA结构和RNA间互作，对基因表达研究至关重要。\n",
      "3. [Top biopharmas’ zero-carbon drive](https://www.nature.com/articles/s41587-023-02102-1)：推荐此项目的原因是它展示了生物制药行业在实现零碳排放方面的努力，这对于推动整个行业的可持续发展具有示范作用。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 17:50:08.701 | INFO     | metagpt.utils.cost_manager:update_cost:48 - Total running cost: $0.045 | Max budget: $10.000 | Current cost: $0.045, prompt_tokens: 1068, completion_tokens: 1159\n"
     ]
    }
   ],
   "source": [
    "test_act = AnalysisOSSTrending()\n",
    "\n",
    "articles = parse_rss(xml)\n",
    "resp = await test_act.run(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OSS assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 14:17:43.149 | INFO     | metagpt.config:get_default_llm_provider_enum:124 - LLMProviderEnum.OPENAI Model: gpt-4-1106-preview\n",
      "2024-02-04 14:17:43.150 | INFO     | metagpt.config:get_default_llm_provider_enum:126 - API: LLMProviderEnum.OPENAI\n",
      "2024-02-04 14:17:43.167 | INFO     | metagpt.config:get_default_llm_provider_enum:124 - LLMProviderEnum.OPENAI Model: gpt-4-1106-preview\n",
      "2024-02-04 14:17:43.168 | INFO     | metagpt.config:get_default_llm_provider_enum:126 - API: LLMProviderEnum.OPENAI\n"
     ]
    }
   ],
   "source": [
    "from metagpt.team import Team\n",
    "from bigpt.roles.OSS import SubscriptionAssistant, CrawlerEngineer\n",
    "\n",
    "team = Team()\n",
    "team.hire([SubscriptionAssistant(), CrawlerEngineer()])\n",
    "#team.hire([CrawlerEngineer()])\n",
    "team.run_project(\"从36kr创投平台https://pitchhub.36kr.com/financing-flash爬取所有初创企业融资的信息，获取标题，链接， 时间，总结今天的融资新闻，然后在11:40发送给我\")\n",
    "#team.run_project(\"从Nature期刊(https://www.nature.com/nature/research-articles)爬取所有微生物学领域中与生物信息学、新方法、新发现相关的信息，获取标题，链接， 时间，总结今天的科研新进展，在14:20分发送给我\")\n",
    "#asyncio.run(team.run())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 14:17:45.986 | INFO     | metagpt.config:get_default_llm_provider_enum:124 - LLMProviderEnum.OPENAI Model: gpt-4-1106-preview\n",
      "2024-02-04 14:17:45.988 | INFO     | metagpt.config:get_default_llm_provider_enum:126 - API: LLMProviderEnum.OPENAI\n",
      "2024-02-04 14:17:46.006 | INFO     | bigpt.roles.OSS:_act:62 - Grace(Subscription Assistant): ready to ParseSubRequirement\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONTENT]\n",
      "{\n",
      "    \"Language\": \"zh_cn\",\n",
      "    \"Cron Expression\": \"40 11 * * *\",\n",
      "    \"Crawler URL List\": [\n",
      "        \"https://pitchhub.36kr.com/financing-flash\"\n",
      "    ],\n",
      "    \"Page Content Extraction\": \"获取所有初创企业当天融资的标题，链接和时间。\",\n",
      "    \"Crawl Post Processing\": \"总结今天的融资新闻。\",\n",
      "    \"Information Supplement"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 14:17:52.513 | INFO     | metagpt.utils.cost_manager:update_cost:48 - Total running cost: $0.126 | Max budget: $10.000 | Current cost: $0.008, prompt_tokens: 490, completion_tokens: 109\n",
      "2024-02-04 14:17:52.541 | INFO     | metagpt.roles.role:_act:360 - John(Crawling Engineer): to do WriteCrawlerCode(WriteCrawlerCode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\": \"\"\n",
      "}\n",
      "[/CONTENT]\n",
      "To achieve the user requirement of extracting all startup financing titles, links, and times from the given HTML outline, we can write a `parse` function that looks for the specific HTML structure where this information is contained. Based on the provided outline, it seems that each financing news item is contained within a `div` with the class `css-xle9x`. The title and link are within an `a` tag with the class `title`, and the time is within a `span` tag with the class `time`.\n",
      "\n",
      "Here's a complete `parse` function that extracts this information:\n",
      "\n",
      "```python\n",
      "from bs4 import BeautifulSoup\n",
      "from typing import List, Dict\n",
      "\n",
      "def parse(soup: BeautifulSoup) -> List[Dict[str, str]]:\n",
      "    # Find all the divs that contain financing news\n",
      "    news_items = soup.find_all('div', class_='css-xle9x')\n",
      "    \n",
      "    # List to hold all the extracted financing news\n",
      "    financing_news = []\n",
      "    \n",
      "    # Iterate over each news item and extract the title, link, and time\n",
      "    for item in news_items:\n",
      "        title_tag = item.find('a', class_='title')\n",
      "        time_tag = item.find('span', class_='time')\n",
      "        \n",
      "        # Extract title, link, and time if they are available\n",
      "        title = title_tag.get_text(strip=True) if title_tag else None\n",
      "        link = title_tag['href'] if title_tag and title_tag.has_attr('href') else None\n",
      "        time = time_tag.get_text(strip=True) if time_tag else None\n",
      "        \n",
      "        # Add the news item to the list if it has a title and time\n",
      "        if title and time:\n",
      "            financing_news.append({\n",
      "                'title': title,\n",
      "                'link': link,\n",
      "                'time': time\n",
      "            })\n",
      "    \n",
      "    return financing_news\n",
      "\n",
      "# Example usage:\n",
      "# html = ... # the HTML content you want to parse\n",
      "# soup = BeautifulSoup(html, 'html.parser')\n",
      "# news = parse(soup)\n",
      "# print(news)\n",
      "```\n",
      "\n",
      "Please note that this function assumes that the `href` attribute of the `a` tag contains the full URL. If it contains a relative URL, you might need to prepend the base URL of the website to `link`.\n",
      "\n",
      "Also, the function returns a list of dictionaries, where each dictionary represents a financing news item with keys `'title'`, `'link'`, and `'time'`. If the HTML structure is different from the provided outline, you may need to adjust the class names or tag names in the `find` and `find"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 14:18:21.651 | INFO     | metagpt.utils.cost_manager:update_cost:48 - Total running cost: $0.236 | Max budget: $10.000 | Current cost: $0.110, prompt_tokens: 9387, completion_tokens: 524\n",
      "2024-02-04 14:18:21.657 | INFO     | bigpt.roles.OSS:_act:62 - Grace(Subscription Assistant): ready to RunSubscription\n",
      "2024-02-04 14:18:21.751 | INFO     | metagpt.config:get_default_llm_provider_enum:124 - LLMProviderEnum.OPENAI Model: gpt-4-1106-preview\n",
      "2024-02-04 14:18:21.752 | INFO     | metagpt.config:get_default_llm_provider_enum:126 - API: LLMProviderEnum.OPENAI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_all` methods accordingly.\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/utils/common.py:501\u001b[0m, in \u001b[0;36mrole_raise_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m kbi:\n",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/roles/role.py:485\u001b[0m, in \u001b[0;36mRole.run\u001b[0;34m(self, with_message)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m rsp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreact()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Reset the next action to be taken.\u001b[39;00m\n",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/roles/role.py:453\u001b[0m, in \u001b[0;36mRole.react\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc\u001b[38;5;241m.\u001b[39mreact_mode \u001b[38;5;241m==\u001b[39m RoleReactMode\u001b[38;5;241m.\u001b[39mREACT:\n\u001b[0;32m--> 453\u001b[0m     rsp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_react()\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc\u001b[38;5;241m.\u001b[39mreact_mode \u001b[38;5;241m==\u001b[39m RoleReactMode\u001b[38;5;241m.\u001b[39mBY_ORDER:\n",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/roles/role.py:432\u001b[0m, in \u001b[0;36mRole._react\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, will do \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc\u001b[38;5;241m.\u001b[39mtodo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m rsp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_act()  \u001b[38;5;66;03m# 这个rsp是否需要publish_message？\u001b[39;00m\n\u001b[1;32m    433\u001b[0m actions_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/repo/bigpt/bigpt/roles/OSS.py:63\u001b[0m, in \u001b[0;36mSubscriptionAssistant._act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: ready to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc\u001b[38;5;241m.\u001b[39mtodo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc\u001b[38;5;241m.\u001b[39mtodo\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m     64\u001b[0m msg \u001b[38;5;241m=\u001b[39m Message(\n\u001b[1;32m     65\u001b[0m     content\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m     66\u001b[0m     instruct_content\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39minstruct_content,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     sent_from\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     70\u001b[0m )\n",
      "File \u001b[0;32m~/repo/bigpt/bigpt/actions/OSSs.py:216\u001b[0m, in \u001b[0;36mRunSubscription.run\u001b[0;34m(self, msgs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m runner\u001b[38;5;241m.\u001b[39msubscribe(role, CronTrigger(spec), callback)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/subscription.py:100\u001b[0m, in \u001b[0;36mSubscriptionRunner.run\u001b[0;34m(self, raise_exception)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/metagpt/lib/python3.9/asyncio/tasks.py:652\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result, loop)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m CrawAct \u001b[38;5;241m=\u001b[39m WriteCrawlerCode()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PyCallGraph(output\u001b[38;5;241m=\u001b[39mGraphvizOutput(output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOSS.png\u001b[39m\u001b[38;5;124m'\u001b[39m,output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m), config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m team\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#await CrawAct.run(\"从Nature期刊(https://www.nature.com/nature/research-articles)爬取所有微生物学领域中与生物信息学、新方法、新发现相关的信息，获取标题，链接， 时间，总结今天的科研新进展\")\u001b[39;00m\n",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/utils/common.py:487\u001b[0m, in \u001b[0;36mserialize_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/team.py:134\u001b[0m, in \u001b[0;36mTeam.run\u001b[0;34m(self, n_round, idea, send_to, auto_archive)\u001b[0m\n\u001b[1;32m    131\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_round\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m left.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_balance()\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39marchive(auto_archive)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m~/repo/MetaGPT/metagpt/environment.py:131\u001b[0m, in \u001b[0;36mEnvironment.run\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    128\u001b[0m     future \u001b[38;5;241m=\u001b[39m role\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    129\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mfutures)\n\u001b[1;32m    132\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis idle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_idle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pycallgraph2 import Config,PyCallGraph, GlobbingFilter\n",
    "from pycallgraph2.output import GraphvizOutput \n",
    "\n",
    "from bigpt.actions.OSSs import WriteCrawlerCode, ParseSubRequirement, RunSubscription \n",
    "\n",
    "\n",
    "config = Config()\n",
    "#config = Config(max_depth=1)\n",
    "config.trace_filter = GlobbingFilter(exclude=[\n",
    "    'pycallgraph.*',\n",
    "    '*.secret_function',\n",
    "    '__*'\n",
    "])\n",
    "\n",
    "CrawAct = WriteCrawlerCode()\n",
    "\n",
    "with PyCallGraph(output=GraphvizOutput(output_file='OSS.png',output_type='png'), config=config):\n",
    "    await team.run()\n",
    "    \n",
    "    #await CrawAct.run(\"从Nature期刊(https://www.nature.com/nature/research-articles)爬取所有微生物学领域中与生物信息学、新方法、新发现相关的信息，获取标题，链接， 时间，总结今天的科研新进展\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metagpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
